

#  Public035.pdf


Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
Thuáº­t toÃ¡n linear regression giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n cÃ³ Ä‘áº§u ra lÃ  giÃ¡ trá»‹ thá»±c, vÃ­ dá»¥: dá»±
Ä‘oÃ¡n giÃ¡ nhÃ , dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u, dá»± Ä‘oÃ¡n tuá»•i,...
1. BÃ i toÃ¡n
Báº¡n lÃ m á»Ÿ cÃ´ng ty báº¥t Ä‘á»™ng sáº£n, báº¡n cÃ³ dá»¯ liá»‡u vá» diá»‡n tÃ­ch vÃ  giÃ¡ nhÃ , giá» cÃ³ má»™t ngÃ´i
nhÃ  má»›i báº¡n muá»‘n Æ°á»›c tÃ­nh xem giÃ¡ ngÃ´i nhÃ  Ä‘Ã³ khoáº£ng bao nhiÃªu. TrÃªn thá»±c táº¿ thÃ¬ giÃ¡
nhÃ  phá»¥ thuá»™c ráº¥t nhiá»u yáº¿u tá»‘: diá»‡n tÃ­ch, sá»‘ phÃ²ng, gáº§n trung tÃ¢m thÆ°Æ¡ng máº¡i,.. nhÆ°ng
Ä‘á»ƒ cho bÃ i toÃ¡n Ä‘Æ¡n giáº£n giáº£ sá»­ giÃ¡ nhÃ  chá»‰ phá»¥ thuá»™c vÃ o diá»‡n tÃ­ch cÄƒn nhÃ . Báº¡n cÃ³ dá»¯
liá»‡u vá» diá»‡n tÃ­ch vÃ  giÃ¡ bÃ¡n cá»§a 30 cÄƒn nhÃ  nhÆ° sau:
Diá»‡n tÃ­ch(m2) GiÃ¡ bÃ¡n (triá»‡u VNÄ)
30 448.524


# 509.248




# 535.104




# 551.432




# 623.418


... ...
Khi cÃ³ dá»¯ liá»‡u mÃ¬nh sáº½ visualize dá»¯ liá»‡u lÃªn hÃ¬nh 3.1
Náº¿u giá» yÃªu cáº§u báº¡n Æ°á»›c lÆ°á»£ng nhÃ  50 mÃ©t vuÃ´ng khoáº£ng bao nhiÃªu tiá»n thÃ¬ báº¡n sáº½ lÃ m
tháº¿ nÃ o? Váº½ má»™t Ä‘Æ°á»ng tháº³ng gáº§n vá»›i cÃ¡c Ä‘iá»ƒm trÃªn nháº¥t vÃ  tÃ­nh giÃ¡ nhÃ  á»Ÿ Ä‘iá»ƒm 50 nhÆ°
á»Ÿ hÃ¬nh 3.2
Vá» máº·t láº­p trÃ¬nh cÅ©ng cáº§n lÃ m 2 viá»‡c nhÆ° váº­y:
. Training: TÃ¬m Ä‘Æ°á»ng tháº³ng (model) gáº§n cÃ¡c Ä‘iá»ƒm trÃªn nháº¥t. Má»i ngÆ°á»i cÃ³ thá»ƒ váº½ ngay
Ä‘Æ°á»£c Ä‘Æ°á»ng tháº³ng mÃ´ táº£ dá»¯ liá»‡u tá»« hÃ¬nh 1, nhÆ°ng mÃ¡y tÃ­nh thÃ¬ khÃ´ng, nÃ³ pháº£i Ä‘i tÃ¬m
báº±ng thuáº­t toÃ¡n Gradient descent á»Ÿ phÃ­a dÆ°á»›i. (Tá»« model vÃ  Ä‘Æ°á»ng tháº³ng Ä‘Æ°á»£c dÃ¹ng
thay tháº¿ láº«n nhau trong pháº§n cÃ²n láº¡i cá»§a bÃ i nÃ y).
m2 . Prediction: Dá»± Ä‘oÃ¡n xem giÃ¡ cá»§a ngÃ´i nhÃ  50 cÃ³ giÃ¡ bao nhiÃªu dá»±a trÃªn Ä‘Æ°á»ng tÃ¬m
Ä‘Æ°á»£c á»Ÿ pháº§n trÃªn.
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
HÃ¬nh 3.1: Äá»“ thá»‹ quan há»‡ giá»¯a diá»‡n tÃ­ch vÃ  giÃ¡ nhÃ .
2. Thiáº¿t láº­p cÃ´ng thá»©c


# Model


PhÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng cÃ³ dáº¡ng y = ax+b vÃ­ dá»¥ hÃ¬nh 3.3. Thay vÃ¬ dÃ¹ng kÃ­ hiá»‡u a, b
cho phÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng; Ä‘á»ƒ tiá»‡n cho biá»ƒu diá»…n ma tráº­n pháº§n sau ta sáº½ thay w1 =
a,w0 = b
NÃªn phÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»£c viáº¿t láº¡i thÃ nh: y = w1âˆ—x+w0 => Viá»‡c tÃ¬m Ä‘Æ°á»ng tháº³ng giá» thÃ nh
viá»‡c tÃ¬m w0,w1.
Äá»ƒ tiá»‡n cho viá»‡c thiáº¿t láº­p cÃ´ng thá»©c, ta sáº½ Ä‘áº·t kÃ½ hiá»‡u cho dá»¯ liá»‡u á»Ÿ báº£ng dá»¯ liá»‡u: (x1,y1)
= (30, 448.524), (x2,y2) = (32.4138, 509.248),..
Tá»©c lÃ  nhÃ  diá»‡n tÃ­ch xi thá»±c sá»± cÃ³ giÃ¡ yi . CÃ²n giÃ¡ trá»‹ mÃ  model hiá»‡n táº¡i Ä‘ang dá»± Ä‘oÃ¡n kÃ­
hiá»‡u lÃ  yË†i = w1âˆ—xi +w0


# Loss function


Viá»‡c tÃ¬m w0,w1 cÃ³ thá»ƒ Ä‘Æ¡n giáº£n náº¿u lÃ m báº±ng máº¯t nhÆ°ng mÃ¡y tÃ­nh khÃ´ng biáº¿t Ä‘iá»u Ä‘áº¥y,
nÃªn ban Ä‘áº§u giÃ¡ trá»‹ Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn vÃ­ dá»¥ w0 = 0,w1 = 1 sau Ä‘áº¥y Ä‘Æ°á»£c chá»‰nh dáº§n.
RÃµ rÃ ng cÃ³ thá»ƒ tháº¥y Ä‘Æ°á»ng y = x nhÆ° á»Ÿ hÃ¬nh 3.4 khÃ´ng há» gáº§n cÃ¡c Ä‘iá»ƒm hay khÃ´ng pháº£i
m2 lÃ  Ä‘Æ°á»ng mÃ  ta cáº§n tÃ¬m. VÃ­ dá»¥ táº¡i Ä‘iá»ƒm x = 42 (nhÃ  42 ) giÃ¡ tháº­t lÃ  625 triá»‡u nhÆ°ng
giÃ¡ mÃ  model dá»± Ä‘oÃ¡n chá»‰ lÃ  42 triá»‡u.
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
m2 HÃ¬nh 3.2: Æ¯á»›c tÃ­nh giÃ¡ cÄƒn nhÃ  50
HÃ¬nh 3.4: Sá»± khÃ¡c nhau táº¡i Ä‘iá»ƒm x = 42 cá»§a model Ä‘Æ°á»ng tháº³ng y = x vÃ  giÃ¡ trá»‹ thá»±c táº¿
á»Ÿ báº£ng 1
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
HÃ¬nh 3.3: VÃ­ dá»¥ Ä‘Æ°á»ng tháº³ng y = x + 1 (a = 1 vÃ  b = 1)
NÃªn giá» cáº§n 1 hÃ m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ lÃ  Ä‘Æ°á»ng tháº³ng vá»›i bá»™ tham sá»‘ (w0,w1) = (0,1) cÃ³ tá»‘t hay
khÃ´ng. Vá»›i má»—i Ä‘iá»ƒm dá»¯ liá»‡u (xi,yi) Ä‘á»™ chÃªnh lá»‡ch giá»¯a giÃ¡ tháº­t vÃ  giÃ¡ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tÃ­nh
1
báº±ng: (ğ‘¦Ì‚ğ‘–âˆ’ğ‘¦ğ‘–)2.
2
VÃ  Ä‘á»™ chÃªnh lá»‡ch trÃªn toÃ n bá»™ dá»¯ liá»‡u tÃ­nh báº±ng tá»•ng chÃªnh lá»‡ch cá»§a tá»«ng Ä‘iá»ƒm:
1 1 ğ‘â‹…âˆ‘ğ‘ (ğ‘¦Ì‚ğ‘–âˆ’ğ‘¦ğ‘–)2
(N lÃ  sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u). Nháº­n xÃ©t: ğ½= â‹…
ğ‘–=1
2
J khÃ´ng Ã¢m
â—
J cÃ ng nhá» thÃ¬ Ä‘Æ°á»ng tháº³ng cÃ ng gáº§n Ä‘iá»ƒm dá»¯ liá»‡u. Náº¿u J = 0 thÃ¬ Ä‘Æ°á»ng tháº³ng Ä‘i
â—
qua táº¥t cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u.
J Ä‘Æ°á»£c gá»i lÃ  loss function, hÃ m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ xem bá»™ tham sá»‘ hiá»‡n táº¡i cÃ³ tá»‘t vá»›i dá»¯ liá»‡u
khÃ´ng.
=> BÃ i toÃ¡n tÃ¬m Ä‘Æ°á»ng tháº³ng gáº§n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nháº¥t trá»Ÿ thÃ nh tÃ¬m w0,w1 sao cho hÃ m
J Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t.
TÃ³m táº¯t: Cáº§n tÃ¬m Ä‘Æ°á»ng tháº³ng (model) fit nháº¥t vá»›i dá»¯ liá»‡u, tÆ°Æ¡ng á»©ng vá»›i viá»‡c tÃ¬m tham
sá»‘ w0,w1 Ä‘á»ƒ cá»±c tiá»ƒu hÃ³a hÃ m J.
Giá» cáº§n má»™t thuáº­t toÃ¡n Ä‘á»ƒ tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t cá»§a hÃ m J(w0, w1). ÄÃ³ chÃ­nh lÃ  thuáº­t toÃ¡n
gradient descent.
3. Thuáº­t toÃ¡n gradient descent


# Äáº¡o hÃ m lÃ  gÃ¬?


x2 CÃ³ nhiá»u ngÆ°á»i cÃ³ thá»ƒ tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m cá»§a hÃ m f(x) = hay f(x) = sin(cos(x)) nhÆ°ng
váº«n khÃ´ng biáº¿t thá»±c sá»± Ä‘áº¡o hÃ m lÃ  gÃ¬. Theo tiáº¿ng hÃ¡n Ä‘áº¡o lÃ  con Ä‘Æ°á»ng, hÃ m lÃ  hÃ m sá»‘
nÃªn Ä‘áº¡o hÃ m chá»‰ sá»± biáº¿n Ä‘á»•i cá»§a hÃ m sá»‘ hay cÃ³ tÃªn thÃ¢n thÆ°Æ¡ng hÆ¡n lÃ  Ä‘á»™ dá»‘c cá»§a Ä‘á»“
thá»‹.
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
x2 HÃ¬nh 3.5: Äá»“ thá»‹ y =
x2 NhÆ° má»i ngÆ°á»i Ä‘Ã£ há»c Ä‘áº¡o hÃ m f(x) = lÃ  fâ€™ (x) = 2âˆ—x (hoÃ n toÃ n cÃ³ thá»ƒ chá»©ng minh tá»«
Ä‘á»‹nh nghÄ©a nhÆ°ng cáº¥p 3 má»i ngÆ°á»i Ä‘Ã£ há»c quÃ¡ nhiá»u vá» cÃ´ng thá»©c nÃªn tÃ´i khÃ´ng Ä‘á» cáº­p
láº¡i). Nháº­n xÃ©t:
fâ€™(1) = 2 * 1 < fâ€™(2) = 2 * 2 nÃªn má»i ngÆ°á»i cÃ³ thá»ƒ tháº¥y trÃªn hÃ¬nh lÃ  Ä‘á»“ thá»‹ táº¡i Ä‘iá»ƒm
â—
x = 2 dá»‘c hÆ¡n Ä‘á»“ thá»‹ táº¡i Ä‘iá»ƒm x = 1, tuy nhiÃªn fâ€™(-2) = -4 < fâ€™(-1) = -2 nhÆ°ng Ä‘á»“ thá»‹ táº¡i x
= -2 dá»‘c hÆ¡n Ä‘á»“ thá»‹ táº¡i x = -1 => trá»‹ tuyá»‡t Ä‘á»‘i cá»§a Ä‘áº¡o hÃ m táº¡i má»™t Ä‘iá»ƒm cÃ ng lá»›n thÃ¬ Ä‘á»“
thá»‹ táº¡i Ä‘iá»ƒm Ä‘áº¥y cÃ ng dá»‘c.
fâ€™(-1) = 2 * (-1) = -2 < 0 => Ä‘á»“ thá»‹ Ä‘ang giáº£m hay khi tÄƒng x thÃ¬ y sáº½ giáº£m; ngÆ°á»£c
â—
láº¡i Ä‘áº¡o hÃ m táº¡i Ä‘iá»ƒm nÃ o Ä‘Ã³ mÃ  dÆ°Æ¡ng thÃ¬ Ä‘á»“ thá»‹ táº¡i Ä‘iá»ƒm Ä‘áº¥y Ä‘ang tÄƒng.


# Gradient descent


Gradient descent lÃ  thuáº­t toÃ¡n tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t cá»§a hÃ m sá»‘ f(x) dá»±a trÃªn Ä‘áº¡o hÃ m.
Thuáº­t toÃ¡n:
1. Khá»Ÿi táº¡o giÃ¡ trá»‹ x = x0 tÃ¹y Ã½
2. GÃ¡n x = x - learning_rate * fâ€™(x) ( learning_rate lÃ  háº±ng sá»‘ dÆ°Æ¡ng vÃ­ dá»¥
learning_rate = 0.001) 3. TÃ­nh láº¡i f(x): Náº¿u f(x) Ä‘á»§ nhá» thÃ¬ dá»«ng láº¡i, ngÆ°á»£c láº¡i tiáº¿p tá»¥c
bÆ°á»›c 2
Thuáº­t toÃ¡n sáº½ láº·p láº¡i bÆ°á»›c 2 má»™t sá»‘ láº§n Ä‘á»§ lá»›n (100 hoáº·c 1000 láº§n tÃ¹y vÃ o bÃ i toÃ¡n vÃ 
há»‡ sá»‘ learning_rate) cho Ä‘áº¿n khi f(x) Ä‘áº¡t giÃ¡ trá»‹ Ä‘á»§ nhá». VÃ­ dá»¥ cáº§n tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t
x2, hÃ m y = hÃ m nÃ y ai cÅ©ng biáº¿t lÃ  giÃ¡ giÃ¡ trá»‹ nhá» nháº¥t lÃ  0 táº¡i x = 0 nhÆ°ng Ä‘á»ƒ cho má»i
ngÆ°á»i dá»… hÃ¬nh dung hÆ¡n vá» thuáº­t toÃ¡n Gradient descent nÃªn tÃ´i láº¥y vÃ­ dá»¥ Ä‘Æ¡n giáº£n.
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
HÃ¬nh 3.6: VÃ­ dá»¥ vá» thuáº­t toÃ¡n gradient descent
1. BÆ°á»›c 1: Khá»Ÿi táº¡o giÃ¡ trá»‹ ngáº«u nhiÃªn x = -2 (Ä‘iá»ƒm A).
2. BÆ°á»›c 2: Do á»Ÿ A Ä‘á»“ thá»‹ giáº£m nÃªn fâ€™(x=-2) = 2*(-2) = -4 < 0 => Khi gÃ¡n x = x -
learning_rate * fâ€™(x) nÃªn x tÄƒng nÃªn Ä‘á»“ thá»‹ bÆ°á»›c tiáº¿p theo á»Ÿ Ä‘iá»ƒm C. Tiáº¿p tá»¥c thá»±c hiá»‡n
bÆ°á»›c 2, gÃ¡n x = x learning_rate * fâ€™(x) thÃ¬ Ä‘á»“ thá»‹ á»Ÿ Ä‘iá»ƒm D,... => hÃ m sá»‘ giáº£m dáº§n dáº§n
tiáº¿n tá»›i giÃ¡ trá»‹ nhá» nháº¥t.
Má»i ngÆ°á»i cÃ³ Ä‘á»ƒ Ã½ lÃ  trá»‹ tuyá»‡t Ä‘á»‘i cá»§a Ä‘áº¡o hÃ m táº¡i A lá»›n hÆ¡n táº¡i C vÃ  táº¡i C lá»›n hÆ¡n táº¡i
D khÃ´ng? Äáº¿n khi Ä‘áº¿n gáº§n Ä‘iá»ƒm Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t x = 0, thÃ¬ Ä‘áº¡o hÃ m xáº¥p xá»‰ 0 Ä‘áº¿n khi
hÃ m Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t táº¡i x = 0, thÃ¬ Ä‘áº¡o hÃ m báº±ng 0, nÃªn táº¡i Ä‘iá»ƒm gáº§n giÃ¡ trá»‹ nhá» nháº¥t
thÃ¬ bÆ°á»›c 2 gÃ¡n x = x - learning_rate * fâ€™(x) lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ vÃ  gáº§n nhÆ° lÃ  giá»¯ nguyÃªn
giÃ¡ trá»‹ cá»§a x.
TÆ°Æ¡ng tá»± náº¿u giÃ¡ trá»‹ khá»Ÿi táº¡o táº¡i x = 2 (táº¡i B) thÃ¬ Ä‘áº¡o hÃ m táº¡i B dÆ°Æ¡ng nÃªn do x = x -
learning_rate * fâ€™(x) giáº£m -> Ä‘á»“ thá»‹ á»Ÿ Ä‘iá»ƒm E -> rá»“i tiáº¿p tá»¥c gÃ¡n x=x -learning_rate *
fâ€™(x) thÃ¬ hÃ m f(x) cÅ©ng sáº½ giáº£m dáº§n dáº§n Ä‘áº¿n giÃ¡ trá»‹ nhá» nháº¥t.
VÃ­ dá»¥: chá»n x = 10, learning_rate = 0.1, bÆ°á»›c 2 sáº½ cáº­p nháº¥t x = x - learning_rate*fâ€™(x)
x2 = x learning_rate *2*x, giÃ¡ trá»‹ f(x) = sáº½ thay Ä‘á»•i qua cÃ¡c láº§n thá»±c hiá»‡n bÆ°á»›c 2 nhÆ° sau:
Láº§n x f(x)
1 8.00 64.00
2 6.40 40.96
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
3 5.12 26.21
4 4.10 16.78
5 3.28 10.74
6 2.62 6.87
7 2.10 4.40
8 1.68 2.81
9 1.34 1.80
10 1.07 1.15
HÃ¬nh 3.7: VÃ­ dá»¥ vá» thuáº­t toÃ¡n gradient descent
Nháº­n xÃ©t:
Thuáº­t toÃ¡n hoáº¡t Ä‘á»™ng ráº¥t tá»‘t trong trÆ°á»ng há»£p khÃ´ng thá»ƒ tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t báº±ng
â—
Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh.
Viá»‡c quan trá»ng nháº¥t cá»§a thuáº­t toÃ¡n lÃ  tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m sá»‘ theo tá»«ng biáº¿n
â—
sau Ä‘Ã³ láº·p láº¡i bÆ°á»›c 2.
Viá»‡c chá»n há»‡ sá»‘ learning_rate cá»±c kÃ¬ quan trá»ng, cÃ³ 3 trÆ°á»ng há»£p:
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
Náº¿u learning_rate nhá»: má»—i láº§n hÃ m sá»‘ giáº£m ráº¥t Ã­t nÃªn cáº§n ráº¥t nhiá»u láº§n thá»±c
â—
hiá»‡n bÆ°á»›c 2 Ä‘á»ƒ hÃ m sá»‘ Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t.
Náº¿u learning_rate há»£p lÃ½: sau má»™t sá»‘ láº§n láº·p bÆ°á»›c 2 vá»«a pháº£i thÃ¬ hÃ m sáº½ Ä‘áº¡t giÃ¡
â—
trá»‹ Ä‘á»§ nhá».
Náº¿u learning_rate quÃ¡ lá»›n: sáº½ gÃ¢y hiá»‡n tÆ°á»£ng overshoot (nhÆ° trong hÃ¬nh 3.8) vÃ 
â—
khÃ´ng bao giá» Ä‘áº¡t Ä‘Æ°á»£c giÃ¡ trá»‹ nhá» nháº¥t cá»§a hÃ m.
HÃ¬nh 3.8: CÃ¡c giÃ¡ trá»‹ learning_rate khÃ¡c nhau [13]
CÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ kiá»ƒm tra learning_rate há»£p lÃ½ hay khÃ´ng lÃ  kiá»ƒm tra giÃ¡ trá»‹ hÃ m f(x) sau
má»—i láº§n thá»±c hiá»‡n bÆ°á»›c 2 báº±ng cÃ¡ch váº½ Ä‘á»“ thá»‹ vá»›i trá»¥c x lÃ  sá»‘ láº§n thá»±c hiá»‡n bÆ°á»›c 2, trá»¥c
y lÃ  giÃ¡ trá»‹ loss function tÆ°Æ¡ng á»©ng á»Ÿ bÆ°á»›c Ä‘áº¥y.
Há»’I QUY TUYáº¾N TÃNH
Láº§n ban hÃ nh: 1
HÃ¬nh 3.9: Loss lÃ  giÃ¡ trá»‹ cá»§a hÃ m cáº§n tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t, Epoch á»Ÿ Ä‘Ã¢y lÃ  sá»‘ cáº§n thá»±c
hiá»‡n bÆ°á»›c 2 [13]
4. So sÃ¡nh cÃ¡c loss function


# Mean Absolute Error, L1 Loss


Mean Absolute Error (MAE) hay cÃ²n Ä‘Æ°á»£c gá»i lÃ  L1 Loss lÃ  má»™t loss function Ä‘Æ°á»£c sá»­
dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh há»“i quy, Ä‘áº·c biá»‡t cho cÃ¡c mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh. MAE Ä‘Æ°á»£c
tÃ­nh báº±ng tá»•ng cÃ¡c trá»‹ tuyá»‡t Ä‘á»‘i cá»§a hiá»‡u giá»¯a giÃ¡ trá»‹ thá»±c (yi: target) vÃ  giÃ¡ trá»‹ mÃ  mÃ´
hÃ¬nh cá»§a chÃºng ra dá»± Ä‘oÃ¡n (ğ‘¦Ì‚ğ‘–: predicted).
ğ‘
1
|ğ‘¦Ì‚ğ‘–âˆ’ğ‘¦ğ‘–| ğ‘€ğ´ğ¸= ğ‘âˆ‘
ğ‘–=1


# Mean Square Error, L2 Loss


Mean Square Error (MSE) hay cÃ²n Ä‘Æ°á»£c gá»i lÃ  L2 Loss lÃ  má»™t loss function cÅ©ng Ä‘Æ°á»£c
sá»­ dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh há»“i quy, Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh. MSE Ä‘Æ°á»£c
tÃ­nh báº±ng tá»•ng cÃ¡c bÃ¬nh phÆ°Æ¡ng cá»§a hiá»‡u giá»¯a giÃ¡ trá»‹ thá»±c (yi: target) vÃ  giÃ¡ trá»‹ mÃ  mÃ´
hÃ¬nh cá»§a chÃºng ra dá»± Ä‘oÃ¡n (ğ‘¦Ì‚ğ‘–: predicted).
ğ‘
1
(ğ‘¦Ì‚ğ‘–âˆ’ğ‘¦ğ‘–)2 ğ‘€ğ‘†ğ¸= ğ‘âˆ‘
ğ‘–=1