

#  Public046.pdf


1. Vectorization
Ngay tá»« bÃ i Ä‘áº§u tiÃªn vá» linear regression, tÃ´i Ä‘Ã£ giá»›i thiá»‡u vá» viá»‡c biá»ƒu diá»…n vÃ  tÃ­nh toÃ¡n dÆ°á»›i
dáº¡ng vector. Viá»‡c biá»ƒu diá»…n bÃ i toÃ¡n dÆ°á»›i dáº¡ng vector nhÆ° váº­y gá»i lÃ  vectorization. NÃ³ khÃ´ng
chá»‰ giÃºp code gá»n láº¡i mÃ  cÃ²n tÄƒng tá»‘c Ä‘á»™ tÃ­nh Ä‘oÃ¡n má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ khi thá»±c hiá»‡n cÃ¡c phÃ©p
tÃ­nh trÃªn vector, ma tráº­n so vá»›i for-loop.
# -*- coding: utf-8 -*-
import numpy as np import time
a = np.random.rand(10000000)
b = np.random.rand(10000000)
start = time.time()
c = np.dot(a, b.T)
end = time.time()
print("Vectorization time : ", end-start)
start = time.time()
c = 0
for i in range(len(a)):
c += a[i] * b[i]
end = time.time()
print("For-loop time : ", end-start)
Báº¡n sáº½ tháº¥y náº¿u dÃ¹ng for-loop máº¥t hÆ¡n 4s Ä‘á»ƒ nhÃ¢n 2 vector trong khi dÃ¹ng thÆ° viá»‡n numpy Ä‘á»ƒ
tÃ­nh chá»‰ máº¥t 0.01s. Táº¡i sao láº¡i nhÆ° tháº¿ nhá»‰?
Giáº£ sá»­ báº¡n cÃ³ 10 táº¥n hÃ ng cáº§n váº­n chuyá»ƒn tá»« A Ä‘áº¿n B vÃ  báº¡n cÃ³ má»™t xe táº£i vá»›i kháº£ nÄƒng chá»Ÿ
Ä‘Æ°á»£c 5 táº¥n má»—i láº§n. Váº­y náº¿u cháº¥t Ä‘áº§y hÃ ng má»—i láº§n chá»Ÿ thÃ¬ chá»‰ cáº§n 2 láº§n cháº¡y lÃ  chuyá»ƒn háº¿t sá»‘
hÃ ng. Tuy nhiÃªn náº¿u má»—i láº§n báº¡n chá»‰ cháº¥t 1 táº¥n hÃ ng lÃªn xe Ä‘á»ƒ chá»Ÿ Ä‘i thÃ¬ xe cáº§n Ä‘i tá»›i 10 láº§n
Ä‘á»ƒ chuyá»ƒn háº¿t sá»‘ hÃ ng.
TÆ°Æ¡ng tá»± nhÆ° váº­y, khi nhÃ¢n 2 vector á»Ÿ trÃªn báº¡n cáº§n thá»±c hiá»‡n 10000000 phÃ©p tÃ­nh nhÃ¢n 2 sá»‘.
Giáº£ sá»­ mÃ¡y tÃ­nh cÃ³ thá»ƒ tÃ­nh Ä‘Æ°á»£c tá»‘i Ä‘a 1000 phÃ©p tÃ­nh nhÃ¢n má»™t lÃºc. Viá»‡c báº¡n dÃ¹ng for-loop
giá»‘ng nhÆ° má»—i thá»i Ä‘iá»ƒm báº¡n chá»‰ yÃªu cáº§u mÃ¡y tÃ­nh thá»±c hiá»‡n má»™t phÃ©p nhÃ¢n, nÃªn Ä‘á»ƒ nhÃ¢n 2
vector sáº½ cáº§n 10000000 Ä‘Æ¡n vá»‹ thá»i gian. Tuy nhiÃªn thÆ° viá»‡n numpy sáº½ tá»‘i Æ°u viá»‡c tÃ­nh toÃ¡n
báº±ng cÃ¡ch yÃªu cáº§u mÃ¡y tÃ­nh thá»±c hiá»‡n 1000 phÃ©p tÃ­nh má»™t lÃºc, tá»©c lÃ  chá»‰ cáº§n =10000
Ä‘Æ¡n vá»‹ thá»i gian Ä‘á»ƒ hoÃ n thÃ nh phÃ©p nhÃ¢n 2 vector. Váº­y nÃªn lÃ  viá»‡c vectorization thÃ´ng thÆ°á»ng
sáº½ tÃ­nh toÃ¡n nhanh hÆ¡n.
2. Mini-batch gradient descent
Mini-batch gradient descent lÃ  gÃ¬ 2.1
á» trong thuáº­t toÃ¡n gradient descent, táº¡i bÆ°á»›c thá»© hai khi ta tÃ­nh Ä‘áº¡o hÃ m cá»§a loss function vá»›i
cÃ¡c biáº¿n. Trong bÃ i linear regression, ta dÃ¹ng táº¥t cáº£ cÃ¡c dá»¯ liá»‡u trong dataset Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m
rá»“i cáº­p nháº­t bÆ°á»›c 2:
Thuáº­t toÃ¡n gradient descent cháº¡y tá»‘t nhÆ°ng sá»‘ lÆ°á»£ng dá»¯ liá»‡u trong training set chá»‰ lÃ  30. Tuy
nhiÃªn náº¿u dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c lá»›n nhÆ° áº£nh vÃ  sá»‘ lÆ°á»£ng lá»›n hÆ¡n vÃ­ dá»¥ 5000 thÃ¬ viá»‡c tÃ­nh Ä‘áº¡o
hÃ m vá»›i loss function vá»›i toÃ n bá»™ dá»¯ liá»‡u sáº½ ráº¥t tá»‘n thá»i gian. VÃ  mini-batch gradient descent
ra Ä‘á»i Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘áº¥y.
Dá»±a vÃ o sá»‘ lÆ°á»£ng dá»¯ liá»‡u cho má»—i láº§n thá»±c hiá»‡n bÆ°á»›c 2 trong gradient descent lÃ  ngÆ°á»i ta chia
ra lÃ m 3 loáº¡i:
Batch gradient descent: DÃ¹ng táº¥t cáº£ dá»¯ liá»‡u trong training set cho má»—i láº§n thá»±c hiá»‡n â€¢
bÆ°á»›c tÃ­nh Ä‘áº¡o hÃ m.
Mini-batch gradient descent: DÃ¹ng má»™t pháº§n dá»¯ liá»‡u trong training set cho má»—i láº§n â€¢
thá»±c hiá»‡n bÆ°á»›c tÃ­nh Ä‘áº¡o hÃ m.
Stochastic gradient descent: Chá»‰ dÃ¹ng má»™t dá»¯ liá»‡u trong training set cho má»—i láº§n thá»±c â€¢
hiá»‡n bÆ°á»›c tÃ­nh Ä‘áº¡o hÃ m.
VÃ­ dá»¥ Ä‘iá»ƒm thi Ä‘áº¡i há»c trung bÃ¬nh cá»§a má»™t trÆ°á»ng trung há»c phá»• thÃ´ng lÃ  24 Ä‘iá»ƒm. Batch
gradient descent giá»‘ng nhÆ° tÃ­nh Ä‘iá»ƒm trung bÃ¬nh táº¥t cáº£ há»c sinh thi Ä‘áº¡i há»c nÄƒm nay cá»§a
trÆ°á»ng, con sá»‘ sáº½ tÆ°Æ¡ng Ä‘á»‘i gáº§n 24, vÃ­ dá»¥ 24,5. Mini-batch sáº½ chá»n ngáº«u nhiÃªn má»™t sá»‘ há»c
sinh, vÃ­ dá»¥ 32 há»c sinh Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm trung bÃ¬nh thÃ¬ Ä‘iá»ƒm trung bÃ¬nh sáº½ xa 24 hÆ¡n vÃ­ dá»¥ 22. Tuy
nhiÃªn, Stochastic giá»‘ng nhÆ° chá»‰ chá»n má»™t há»c sinh lÃ m Ä‘iá»ƒm trung bÃ¬nh, thÃ¬ lÃºc nÃ y Ä‘iá»ƒm
trung bÃ¬nh cÃ³ thá»ƒ khÃ¡c xa con sá»‘ 24 ráº¥t nhiá»u, vÃ­ dá»¥ 18 hoáº·c 29. Viá»‡c Ä‘iá»ƒm trung bÃ¬nh á»Ÿ mini-
batch hay stochastic khÃ¡c so vá»›i Ä‘iá»ƒm trung bÃ¬nh toÃ n trÆ°á»ng gá»i lÃ  nhiá»…u trong dá»¯ liá»‡u. Hay
giáº£i thÃ­ch Ä‘Æ¡n giáº£n lÃ  chá»‰ láº¥y 1 hoáº·c má»™t pháº§n dá»¯ liá»‡u thÃ¬ khÃ´ng thá»ƒ mÃ´ táº£ háº¿t Ä‘Æ°á»£c táº¥t cáº£ dá»¯
liá»‡u.
Do Ä‘Ã³ hÃ m loss-function vá»›i há»‡ sá»‘ learning_rate phÃ¹ há»£p thÃ¬ batch gradient descent theo epoch
sáº½ giáº£m Ä‘á»u Ä‘áº·n. VÃ¬ cÃ³ nhiá»…u trong dá»¯ liá»‡u nÃªn mini-batch thÃ¬ váº«n giáº£m nhÆ°ng cÃ³ dao Ä‘á»™ng vÃ 
stochastic cÃ³ giáº£m nhÆ°ng dao Ä‘á»™ng cá»±c kÃ¬ lá»›n.
HÃ¬nh 12.1: So sÃ¡nh loss function khi dÃ¹ng batch vÃ  mini-batch [13]
HÃ¬nh dÆ°á»›i lÃ  biá»ƒu diá»…n biá»‡c cáº­p nháº­t há»‡ sá»‘ trong gradient descent, Ä‘iá»ƒm Ä‘á» lÃ  giÃ¡ trá»‹ nhá» nháº¥t ta
cáº§n tÃ¬m, cÃ¡c Ä‘iá»ƒm á»Ÿ ngoÃ i cÃ¹ng lÃ  giÃ¡ trá»‹ khá»Ÿi táº¡o cá»§a há»‡ sá»‘ trong gradient descent. Ta cÃ³ thá»ƒ
tháº¥y vÃ¬ khÃ´ng cÃ³ nhiá»…u nÃªn batch gradient descent thÃ¬ há»‡ sá»‘ cáº­p nháº­t trá»±c tiáº¿p theo 1 Ä‘Æ°á»ng
tháº³ng. Mini-batch thÃ¬ máº¥t nhiá»u thá»i gian hÆ¡n vÃ  cÃ²n Ä‘i chá»‡ch hÆ°á»›ng tuy nhiÃªn thÃ¬ váº«n Ä‘áº¿n Ä‘Æ°á»£c
Ä‘iá»ƒm Ä‘á». CÃ²n stochastic thÃ¬ Ä‘i khÃ¡ lÃ²ng vÃ²ng Ä‘á»ƒ Ä‘áº¿n Ä‘Æ°á»£c Ä‘iá»ƒm Ä‘á» vÃ  vÃ¬ dá»¯ liá»‡u quÃ¡ nhiá»…u nÃªn
cÃ³ thá»ƒ thuáº­t toÃ¡n gradient descent chá»‰ quanh Ä‘iá»ƒm Ä‘á» mÃ  khÃ´ng Ä‘áº¿n Ä‘Æ°á»£c Ä‘iá»ƒm Ä‘á» (minimum
point).
HÃ¬nh 12.2: Cáº­p nháº­t loss function Ä‘áº¿n minimum point cá»§a cÃ¡c thuáº­t toÃ¡n [13]
Batch gradient descent thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng khi sá»‘ lÆ°á»£ng dá»¯ liá»‡u trong training set nhá» hÆ¡n 2000.
Vá»›i lÆ°á»£ng dá»¯ liá»‡u lá»›n thÃ¬ mini-batch gradient descent Ä‘Æ°á»£c sá»­ dá»¥ng. NÃ³ cÃ³ thá»ƒ giáº£i quyáº¿t Ä‘Æ°á»£c
váº¥n Ä‘á» lÆ°á»£ng dá»¯ liá»‡u quÃ¡ lá»›n nhÆ° trong batch gradient descent, hÆ¡n ná»¯a Ä‘á»¡ nhiá»…u vÃ  cÃ³ thá»ƒ dÃ¹ng
vectorization so vá»›i stochastic gradient descent nÃªn thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong deep learning.
CÃ¡c thÃ´ng sá»‘ trong mini-batch gradient descent 2.2
VÃ­ dá»¥ code trong bÃ i 7, trong bÃ i toÃ¡n phÃ¢n loáº¡i chá»¯ sá»‘ cho dá»¯ liá»‡u MNIST
H = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=32, epochs=10,
verbose=1)
X_train, Y_train lÃ  dá»¯ liá»‡u vÃ  label cho training set. TÆ°Æ¡ng tá»± X_val, Y_val lÃ  dá»¯ liá»‡u cho
validation set.
batch_size: LÃ  size trong mini-batch gradient descent, nghÄ©a lÃ  dÃ¹ng bao nhiÃªu dá»¯ liá»‡u cho
má»—i láº§n tÃ­nh vÃ  cáº­p nháº­t há»‡ sá»‘.
steps_per_epoch: LÃ  bao nhiÃªu láº§n thá»±c hiá»‡n bÆ°á»›c 2 trong gradient descent trong má»—i epoch.
Máº·c Ä‘á»‹nh sáº½ lÃ  sá»‘ lÆ°á»£ng dá»¯ liá»‡u chia cho batch_size. Hiá»ƒu Ä‘Æ¡n giáº£n lÃ  má»—i epoch sáº½ dÃ¹ng háº¿t
cÃ¡c dá»¯ liá»‡u Ä‘á»ƒ tÃ­nh gradient descent. epochs: sá»‘ lÆ°á»£ng epoch thá»±c hiá»‡n trong quÃ¡ trÃ¬nh training.
Váº­y thá»±c sá»± sá»‘ láº§n thá»±c hiá»‡n bÆ°á»›c 2 trong gradient descent trong quÃ¡ trÃ¬nh training lÃ : epochs
- steps_per_epoch.
Lá»i khuyÃªn:
Batch_size nÃªn Ä‘Æ°á»£c chá»n lÃ  sá»‘ mÅ© cá»§a 2 vÃ­ dá»¥ 16, 32, 64, 128 Ä‘á»ƒ CPU/GPU tÃ­nh â€¢
toÃ¡n tá»‘t hÆ¡n. GiÃ¡ trá»‹ máº·c Ä‘á»‹nh lÃ  32.
NÃªn váº½ Ä‘á»“ thá»‹ loss/epoch Ä‘á»ƒ chá»n batch_size phÃ¹ há»£p. â€¢
3. Bias vÃ  variance
Bias, variance lÃ  gÃ¬ 3.1
Bias: nghÄ©a lÃ  Ä‘á»™ lá»‡ch, biá»ƒu thá»‹ sá»± chÃªnh lá»‡ch giá»¯a giÃ¡ trá»‹ trung bÃ¬nh mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n vÃ 
giÃ¡ trá»‹ thá»±c táº¿ cá»§a dá»¯ liá»‡u.
Variance: nghÄ©a lÃ  phÆ°Æ¡ng sai, biá»ƒu thá»‹ Ä‘á»™ phÃ¢n tÃ¡n cá»§a cÃ¡c giÃ¡ trá»‹ mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n so vá»›i
giÃ¡ trá»‹ thá»±c táº¿.
GiÃ¡ trá»‹ tháº­t dá»¯ liá»‡u (ground truth) á»Ÿ giá»¯a tÃ¢m cÃ¡c Ä‘Æ°á»ng trÃ²n. CÃ¡c dáº¥u X lÃ  cÃ¡c giÃ¡ trá»‹ dá»± Ä‘oÃ¡n.
Ta tháº¥y náº¿u high bias thÃ¬ giÃ¡ trá»‹ dá»± Ä‘oÃ¡n ráº¥t xa tÃ¢m. Tuy nhiÃªn náº¿u high variance thÃ¬ cÃ¡c giÃ¡ trá»‹
dá»± Ä‘oÃ¡n phÃ¢n tÃ¡n rá»™ng dáº«n Ä‘áº¿n viá»‡c ra giÃ¡ trá»‹ thá»±c táº¿. => Ta mong muá»‘n low bias vÃ  low
variance.
ÄÃ¢y lÃ  bÃ i toÃ¡n logistic regression, cáº§n tÃ¬m Ä‘Æ°á»ng phÃ¢n chia dá»¯ liá»‡u.
á» hÃ¬nh 1, thÃ¬ Ä‘Æ°á»ng phÃ¢n chia cÃ³ khÃ¡ nhiá»u Ä‘iá»ƒm bá»‹ lá»—i => sá»± chÃªnh lá»‡ch giá»¯a mÃ´ â€¢
hÃ¬nh dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿ cá»§a dá»¯ liá»‡u cao => high bias, hay cÃ²n Ä‘Æ°á»£c gá»i lÃ 
underfitting, Ã½ hiá»ƒu lÃ  mÃ´ hÃ¬nh hiá»‡n táº¡i Ä‘Æ¡n giáº£n hÆ¡n vÃ  chÆ°a mÃ´ táº£ Ä‘Æ°á»£c mÃ´ hÃ¬nh cá»§a dá»¯
liá»‡u thá»±c táº¿.
á» hÃ¬nh 2, Ä‘Æ°á»ng phÃ¢n chia váº«n cÃ³ lá»—i nhÆ°ng á»Ÿ má»©c cháº¥p nháº­n Ä‘Æ°á»£c vÃ  nÃ³ cÃ³ thá»ƒ mÃ´ â€¢
táº£ dá»¯ liá»‡u => low bias, low variance.
á» hÃ¬nh 3, Ä‘Æ°á»ng phÃ¢n chia cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Ãºng táº¥t cáº£ cÃ¡c Ä‘iá»ƒm trong training set â€¢
nhÆ°ng vÃ¬ nÃ³ khÃ´ng tá»•ng quÃ¡t hÃ³a mÃ´ hÃ¬nh dá»¯ liá»‡u thá»±c sá»± nÃªn khi Ã¡p dá»¥ng dá»± Ä‘oÃ¡n vÃ o
validation set thÃ¬ sáº½ cÃ³ ráº¥t nhiá»u lá»—i => high variance hay cÃ²n Ä‘Æ°á»£c gá»i lÃ  overfitting, Ã½ hiá»ƒu
lÃ  mÃ´ hÃ¬nh hiá»‡n táº¡i thá»±c hiá»‡n tá»‘t vá»›i dá»¯ liá»‡u trong training set nhÆ°ng dá»± Ä‘oÃ¡n khÃ´ng tá»‘t vá»›i
validation set. Thá»±c ra khÃ¡i niá»‡m high bias vÃ  high variance khÃ¡ trÃ¬u tÆ°á»£ng vÃ  nhiá»u lÃºc dÃ¹ng
nháº§m láº«n giá»¯a thá»‘ng kÃª vÃ  machine learning. NÃªn khÃ¡i niá»‡m hay Ä‘Æ°á»£c dÃ¹ng hÆ¡n lÃ 
underfitting vÃ  overfitting.
HÃ¬nh 12.3: CÃ¡c Ä‘iá»ƒm mÃ u xanh lÃ  training set, Ä‘iá»ƒm mÃ u Ä‘á» lÃ  validation set
VÃ­ dá»¥ khi luyá»‡n thi Ä‘áº¡i há»c, náº¿u báº¡n chá»‰ luyá»‡n khoáº£ng 1-2 Ä‘á» trÆ°á»›c khi thi thÃ¬ báº¡n sáº½ bá»‹
underfitting vÃ¬ báº¡n chÆ°a hiá»ƒu háº¿t cáº¥u trÃºc, ná»™i dung cá»§a Ä‘á» thi. Tuy nhiÃªn náº¿u báº¡n chá»‰ luyá»‡n kÄ©
50 Ä‘á» tháº§y cÃ´ giÃ¡o báº¡n soáº¡n vÃ  Ä‘Æ°a cho thÃ¬ kháº£ nÄƒng báº¡n sáº½ bá»‹ overfitting vá»›i cÃ¡c Ä‘á» mÃ  tháº§y
cÃ´ giÃ¡o cÃ¡c báº¡n soáº¡n mÃ  khi thi Ä‘áº¡i há»c cÃ³ thá»ƒ Ä‘iá»ƒm sá»‘ cá»§a cÃ¡c báº¡n váº«n tá»‡.
Bias, variance tradeoff 3.2
Náº¿u model quÃ¡ Ä‘Æ¡n giáº£n thÃ¬ ta sáº½ bá»‹ high bias vÃ  low variance. Tuy nhiÃªn náº¿u model quÃ¡ phá»©c
táº¡p thÃ¬ sáº½ bá»‹ high variance vÃ  low bias. Äáº¥y lÃ  bias, variance tradeoff. Do Ä‘Ã³ Ä‘á»ƒ train Ä‘Æ°á»£c
model tá»‘t ta cáº§n cÃ¢n báº±ng giá»¯a bias vÃ  variance.
ÄÃ¡nh giÃ¡ bias and variance 3.3
CÃ³ 2 thÃ´ng sá»‘ thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ bias and variance cá»§a mÃ´ hÃ¬nh lÃ  training set
error vÃ  validation set error. VÃ­ dá»¥ error (1-accuracy) trong logistic regression.
Train set 1% 15% 15% 0.5%
error
Val set error 11% 16% 30% 1%
High High High bias Low bias
variance bias High variance Low variance
Ta mong muá»‘n model lÃ  low bias vÃ  low variance. CÃ¹ng xem má»™t sá»‘ cÃ¡ch Ä‘á»ƒ giáº£i quyáº¿t váº¥n
Ä‘á» high bias hoáº·c high variance nhÃ©.
Giáº£i quyáº¿t high bias (underfitting): Ta cáº§n tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a model
TÄƒng sá»‘ lÆ°á»£ng hidden layer vÃ  sá»‘ node trong má»—i hidden layer. â€¢
DÃ¹ng nhiá»u epoch hÆ¡n Ä‘á»ƒ train model. Giáº£i quyáº¿t high variance (overfitting): â€¢
Thu tháº­p thÃªm dá»¯ liá»‡u hoáº·c dÃ¹ng data augmentation â€¢
DÃ¹ng regularization nhÆ°: L1, L2, dropout â€¢
4. Dropout
Dropout lÃ  gÃ¬ 4.1
Dropout vá»›i há»‡ sá»‘ p nghÄ©a lÃ  trong quÃ¡ trÃ¬nh train model, vá»›i má»—i láº§n thá»±c hiá»‡n cáº­p nháº­t há»‡ sá»‘
trong gradient descent ta ngáº«u nhiÃªn loáº¡i bá» p% sá»‘ lÆ°á»£ng node trong layer Ä‘áº¥y, hay nÃ³i cÃ¡ch
khÃ¡c lÃ  giá»¯ láº¡i (1-p%) node. Má»—i layer cÃ³ thá»ƒ cÃ³ cÃ¡c há»‡ sá»‘ dropout p khÃ¡c nhau.
HÃ¬nh 12.4: So sÃ¡nh model dropout vÃ  neural network thÃ´ng thÆ°á»ng [26]
VÃ­ dá»¥ mÃ´ hÃ¬nh neural network 1-2-1: 1 input layer, 2 hidden layer vÃ  1 output layer. VÃ­ dá»¥
nhÆ° hidden layer 1, ta dÃ¹ng dropout vá»›i p = 0.6, nÃªn chá»‰ giá»¯ láº¡i 2 trÃªn 5 node cho má»—i láº§n cáº­p
nháº­t.
Dropout háº¡n cháº¿ viá»‡c overfitting 4.2
Overfitting lÃ  mÃ´ hÃ¬nh Ä‘ang dÃ¹ng quÃ¡ phá»©c táº¡p so vá»›i mÃ´ hÃ¬nh tháº­t cá»§a dá»¯ liá»‡u. Khi ta dÃ¹ng
dropout nhÆ° hÃ¬nh trÃªn thÃ¬ rÃµ rÃ ng mÃ´ hÃ¬nh bÃªn pháº£i Ä‘Æ¡n giáº£n hÆ¡n => trÃ¡nh overfitting.
ThÃªm vÃ o Ä‘Ã³, vÃ¬ má»—i bÆ°á»›c khi train model thÃ¬ ngáº«u nhiÃªn (1-p%) cÃ¡c node bá»‹ loáº¡i bá» nÃªn model
khÃ´ng thá»ƒ phá»¥ thuá»™c vÃ o báº¥t kÃ¬ node nÃ o cá»§a layer trÆ°á»›c mÃ  thay vÃ o Ä‘Ã³ cÃ³ xu hÆ°á»›ng tráº£i Ä‘á»u
weight, giá»‘ng nhÆ° trong L2 regularization => trÃ¡nh Ä‘Æ°á»£c overfitting.
Lá»i khuyÃªn khi dÃ¹ng dropout 4.3
Há»‡ sá»‘ p nÃªn á»Ÿ khoáº£ng [0.2, 0.5] . Náº¿u p quÃ¡ nhá» thÃ¬ khÃ´ng cÃ³ tÃ¡c dá»¥ng chá»‘ng â€¢
overfitting, tuy nhiÃªn náº¿u p quÃ¡ lá»›n thÃ¬ gáº§n nhÆ° loáº¡i bá» layer Ä‘áº¥y vÃ  cÃ³ thá»ƒ dáº«n Ä‘áº¿n
underfitting.
NÃªn dÃ¹ng model lá»›n, phá»©c táº¡p hÆ¡n vÃ¬ ta cÃ³ dropout chá»‘ng overfitting. â€¢
Dropout chá»‰ nÃªn dÃ¹ng cho Fully Connected layer, Ã­t khi Ä‘Æ°á»£c dÃ¹ng cho ConvNet layer â€¢
Há»‡ sá»‘ p á»Ÿ cÃ¡c layer nÃªn tá»‰ lá»‡ vá»›i sá»‘ lÆ°á»£ng node trong FC layer Ä‘Ã³. â€¢
5. Activation function
Non-linear activation function 5.1
HÃ m activation function Ä‘Æ°á»£c dÃ¹ng sau bÆ°á»›c tÃ­nh tá»•ng linear trong neural network hoáº·c sau
convolutional layer trong CNN. VÃ  hÃ m activation lÃ  non-linear function.
Linear function lÃ  gÃ¬? Theo wiki, "a linear function from the real numbers to the real numbers
is a function whose graph is a line in the plane ", tÃ³m láº¡i linear function lÃ  má»™t Ä‘Æ°á»ng tháº³ng
dáº¡ng y = a*x + b. Váº­y sáº½ ra sao náº¿u hÃ m activation trong neural network lÃ  má»™t linear function?
Giáº£ sá»­ hÃ m activation dáº¡ng y = f(x) = 2*x + 3 vÃ  neural network nhÆ° sau:
HÃ¬nh 12.5: MÃ´ hÃ¬nh neural network, 1-2-1.
z
TÆ°Æ¡ng tá»± a
Do Ä‘Ã³
yË†
x
TÃ³m láº¡i yË† = xâˆ—a+b hay nÃ³i cÃ¡ch khÃ¡c mÃ´ hÃ¬nh neural network chá»‰ lÃ  mÃ´ hÃ¬nh linear
regression Ä‘Æ¡n giáº£n => HÃ m activation function pháº£i lÃ  non-linear function.
Vanishing vÃ  exploding gradient 5.2
Backpropagation lÃ  thuáº­t toÃ¡n Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cÃ¡c há»‡ sá»‘ trong neural network vá»›i
loss function Ä‘á» rá»“i Ã¡p dá»¥ng gradient descent Ä‘á»ƒ tÃ¬m cÃ¡c há»‡ sá»‘.
HÃ¬nh 12.6: MÃ´ hÃ¬nh neural network 2-3-3-1
HÃ¬nh 12.7: QuÃ¡ trÃ¬nh backpropagation
Ta cÃ³ tá»•ng quÃ¡t:
ğ‘›
ğœ•ğ½ ğœ•ğ½
â‹…âˆğ·(ğ‘–)ğ‘Š(ğ‘–) =
ğœ•ğ´Ì‚(ğ‘™) ğœ•ğ‘ŒÌ‚
ğ‘–=ğ‘™+1
Nháº­n xÃ©t:
Náº¿u cÃ¡c há»‡ sá»‘ W vÃ  D Ä‘á»u nhá» hÆ¡n 1 thÃ¬ khi tÃ­nh gradient á»Ÿ cÃ¡c layer Ä‘áº§u ta sáº½ pháº£i â€¢
nhÃ¢n tÃ­ch cá»§a ráº¥t nhiá»u sá»‘ nhá» hÆ¡n 1 nÃªn giÃ¡ trá»‹ sáº½ tiáº¿n dáº§n vá» 0 vÃ  bÆ°á»›c cáº­p nháº­t há»‡ sá»‘ trong
gradient descent trá»Ÿ nÃªn vÃ´ nghÄ©a vÃ  cÃ¡c há»‡ sá»‘ neural network sáº½ khÃ´ng há»c Ä‘Æ°á»£c ná»¯a. =>
Vanishing gradient
Náº¿u cÃ¡c há»‡ sá»‘ W vÃ  D Ä‘á»u lá»›n hÆ¡n 1 thÃ¬ khi tÃ­nh gradient á»Ÿ cÃ¡c layer Ä‘áº§u ta sáº½ pháº£i â€¢
nhÃ¢n tÃ­ch cá»§a ráº¥t nhiá»u sá»‘ lá»›n hÆ¡n 1 nÃªn giÃ¡ trá»‹ sáº½ tiáº¿n dáº§n vá» vÃ´ cÃ¹ng vÃ  bÆ°á»›c cáº­p nháº­t há»‡ sá»‘
trong gradient descent trá»Ÿ nÃªn khÃ´ng chÃ­nh xÃ¡c vÃ  cÃ¡c há»‡ sá»‘ neural network sáº½ khÃ´ng há»c
Ä‘Æ°á»£c ná»¯a. => Exploding gradient
CÃ¡ch giáº£i quyáº¿t vaninshing/exproding gradient lÃ  lá»±a chá»n cÃ¡c giÃ¡ trá»‹ khá»Ÿi táº¡o cho há»‡ sá»‘ phÃ¹
há»£p vÃ  chá»n activation function phÃ¹ há»£p.
Má»™t sá»‘ activation thÃ´ng dá»¥ng 5.3
Sigmoid activation function
HÃ¬nh 12.8: HÃ m sigmoid
1 Äáº¡o hÃ m hÃ m sigmoid lÃ 
ğœ(ğ‘¥) â‹…(1 âˆ’ğœ(ğ‘¥)) â‰¤
4
VÃ­ dá»¥ nÃªn náº¿u báº¡n nhÃ¬n vÃ o cÃ´ng thá»©c (1) á»Ÿ trÃªn thÃ¬ á»Ÿ nhá»¯ng layer Ä‘áº§u tiÃªn
sáº½ bá»‹ vanishing gradient.
Tanh activation function
HÃ¬nh 12.9: HÃ m tanh
ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
HÃ m tanh: ğ‘’ğ‘¥+ğ‘’âˆ’ğ‘¥, giÃ¡ trá»‹ g(x) trong Ä‘oáº¡n (-1,1) ğ‘”(ğ‘¥) =
âˆ’ğ‘”2(ğ‘¥) Äáº¡o hÃ m hÃ m tanh: â‰¤1. Do Ä‘Ã³ khi dÃ¹ng tanh activation function sáº½ bá»‹ vanishing 1
gradient.
ReLU activation function
HÃ¬nh 12.10: HÃ m ReLU
HÃ m relu (rectified linear unit): y = max(0,x) Nháº­n xÃ©t:
HÃ m ReLU activation Ä‘Æ¡n giáº£n Ä‘á»ƒ tÃ­nh => thá»i gian train model nhanh hÆ¡n. â€¢
Äáº¡o hÃ m lÃ  1 vá»›i x >= 0 nÃªn khÃ´ng bá»‹ vanishing gradient. â€¢
Tuy nhiÃªn vá»›i cÃ¡c node cÃ³ giÃ¡ trá»‹ nhá» hÆ¡n 0, qua ReLU activation sáº½ thÃ nh 0, hiá»‡n tÆ°á»£ng Ä‘áº¥y
gá»i lÃ  "Dying ReLU". Náº¿u cÃ¡c node bá»‹ chuyá»ƒn thÃ nh 0 thÃ¬ sáº½ khÃ´ng cÃ³ Ã½ nghÄ©a vá»›i bÆ°á»›c linear
activation á»Ÿ lá»›p tiáº¿p theo vÃ  cÃ¡c há»‡ sá»‘ tÆ°Æ¡ng á»©ng tá»« node Ä‘áº¥y cÅ©ng khÃ´ng Ä‘Æ°á»£c cáº­p nháº­t vá»›i
gradient descent. => Leaky ReLU ra Ä‘á»i.
Leaky ReLU
HÃ¬nh 12.11: HÃ m Leaky ReLU
HÃ m Leaky ReLU cÃ³ cÃ¡c Ä‘iá»ƒm tá»‘t cá»§a hÃ m ReLU vÃ  giáº£i quyáº¿t Ä‘Æ°á»£c váº¥n Ä‘á» Dying ReLU
báº±ng cÃ¡ch xÃ©t má»™t Ä‘á»™ dá»‘c nhá» cho cÃ¡c giÃ¡ trá»‹ Ã¢m thay vÃ¬ Ä‘á»ƒ giÃ¡ trá»‹ lÃ  0.
Lá»i khuyÃªn: Máº·c Ä‘á»‹nh nÃªn dÃ¹ng ReLU lÃ m hÃ m activation. KhÃ´ng nÃªn dÃ¹ng hÃ m sigmoid.
6. Batch Normalize
Má»™t trong nhá»¯ng giáº£ Ä‘á»‹nh chÃ­nh trong Ä‘Æ°á»£c Ä‘Æ°a ra trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh há»c
mÃ¡y Ä‘Ã³ lÃ  phÃ¢n phá»‘i cá»§a dá»¯ liá»‡u Ä‘Æ°á»£c giá»¯ nguyÃªn trong suá»‘t quÃ¡ trÃ¬nh training. Äá»‘i vá»›i cÃ¡c mÃ´
hÃ¬nh tuyáº¿n tÃ­nh, Ä‘Æ¡n giáº£n lÃ  Ã¡nh xáº¡ input vá»›i output thÃ­ch há»£p, Ä‘iá»u kiá»‡n nÃ y luÃ´n Ä‘Æ°á»£c thá»a
mÃ£n. Tuy nhiÃªn, trong trÆ°á»ng há»£p Neural Network vá»›i cÃ¡c lá»›p Ä‘Æ°á»£c xáº¿p chá»“ng lÃªn nhau, áº£nh
hÆ°á»Ÿng cá»§a cÃ¡c hÃ m activation non-linear, Ä‘iá»u kiá»‡n trÃªn khÃ´ng cÃ²n Ä‘Ãºng ná»¯a.
Trong kiáº¿n trÃºc neural network, Ä‘áº§u vÃ o cá»§a má»—i lá»›p phá»¥ thuá»™c nhiá»u vÃ o tham sá»‘ cá»§a toÃ n bá»™
cÃ¡c lá»›p trÆ°á»›c Ä‘Ã³. Háº­u quáº£ lÃ  trong quÃ¡ trÃ¬nh backprop, cÃ¡c trá»ng sá»‘ cá»§a má»™t lá»›p Ä‘Æ°á»£c cáº­p nháº­t
dáº«n Ä‘áº¿n nhá»¯ng thay Ä‘á»•i vá» máº·t dá»¯ liá»‡u sau khi Ä‘i qua lá»›p Ä‘Ã³, nhá»¯ng thay Ä‘á»•i nÃ y bá»‹ khuyáº¿ch
Ä‘áº¡i khi máº¡ng trá»Ÿ nÃªn sÃ¢u hÆ¡n vÃ  cuá»‘i cÃ¹ng lÃ m phÃ¢n phá»‘i cá»§a báº£n Ä‘á»“ Ä‘áº·c trÆ°ng (feature map)
thay Ä‘á»•i, Ä‘Ã¢y Ä‘Æ°á»£c gá»i lÃ  hiá»‡n tÆ°á»£ng covariance shifting. Khi huáº¥n luyá»‡n, cÃ¡c lá»›p luÃ´n pháº£i Ä‘iá»u
chá»‰nh trá»ng sá»‘ Ä‘á»ƒ Ä‘Ã¡p á»©ng nhá»¯ng thay Ä‘á»•i vá» phÃ¢n phá»‘i dá»¯ liá»‡u nháº­n Ä‘Æ°á»£c tá»« cÃ¡c lá»›p trÆ°á»›c, Ä‘iá»u
nÃ y lÃ m cháº­m quÃ¡ trÃ¬nh há»™i tá»¥ cá»§a mÃ´ hÃ¬nh.


# PhÃ¢n tÃ­ch nguyÃªn nhÃ¢n


Váº¥n Ä‘á» 1 : Khi dá»¯ liá»‡u chá»©a nhiá»u thÃ nh pháº§n lá»›n hÆ¡n hoáº·c nhá» hÆ¡n 0 vÃ  khÃ´ng phÃ¢n bá»‘
quanh giÃ¡ trá»‹ trung bÃ¬nh 0 (Non zero mean), káº¿t há»£p vá»›i viá»‡c phÆ°Æ¡ng sai lá»›n (high variance)
lÃ m cho dá»¯ liá»‡u chá»©a nhiá»u thÃ nh pháº§n ráº¥t lá»›n hoáº·c ráº¥t nhá». Trong quÃ¡ trÃ¬nh cáº­p nháº­t trá»ng sá»‘
báº±ng gradient descent, giÃ¡ trá»‹ cá»§a dá»¯ liá»‡u áº£nh hÆ°á»Ÿng trá»±c tiáº¿p lÃªn giÃ¡ trá»‹ Ä‘áº¡o hÃ m (gradient),
do Ä‘Ã³ lÃ m giÃ¡ trá»‹ gradient trá»Ÿ nÃªn quÃ¡ lá»›n hoáº·c qÃºa nhá», nhÆ° chÃºng ta Ä‘Ã£ biáº¿t Ä‘iá»u nÃ y khÃ´ng
há» tá»‘t chÃºt nÃ o. Hiá»‡n tÆ°á»£ng trÃªn xuáº¥t hiá»‡n khÃ¡ phá»• biáº¿n, phá»¥ thuá»™c nhiá»u vÃ o viá»‡c khá»Ÿi táº¡o
trá»ng sá»‘, vÃ  cÃ³ xu hÆ°á»›ng nghiÃªm trá»ng hÆ¡n khi máº¡ng ngÃ y cÃ ng sÃ¢u.
=> Cáº§n má»™t bÆ°á»›c normalize cÃ¡c thÃ nh pháº§n dá»¯ liá»‡u vá» cÃ¹ng mean vÃ  chuáº©n hÃ³a variance.
Váº¥n Ä‘á» 2 : CÃ¡c hÃ m activation non-linear nhÆ° sigmoid, relu, tanh,... Ä‘á»u cÃ³ ngÆ°á»¡ng hay vÃ¹ng
bÃ£o hÃ²a. Khi lan truyá»n tháº³ng, náº¿u dá»¯ liá»‡u cÃ³ cÃ¡c thÃ nh pháº§n quÃ¡ lá»›n hoáº·c quÃ¡ nhá», sau khi Ä‘i
qua cÃ¡c hÃ m activation, cÃ¡c thÃ nh pháº§n nÃ y sáº½ rÆ¡i vÃ o vÃ¹ng bÃ£o hÃ²a vÃ  cÃ³ Ä‘áº§u ra giá»‘ng nhau.
Äiá»u nÃ y dáº«n Ä‘áº¿n luá»“ng dá»¯ liá»‡u sau Ä‘Ã³ trá»Ÿ nÃªn giá»‘ng nhau khi lan truyá»n trong máº¡ng
(covariance
shifting), lÃºc nÃ y cÃ¡c lá»›p cÃ²n láº¡i trong máº¡ng khÃ´ng cÃ²n phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng khÃ¡c
nhau. NgoÃ i ra, Ä‘áº¡o hÃ m táº¡i ngÆ°á»¡ng cá»§a cÃ¡c hÃ m activation báº±ng 0, Ä‘iá»u nÃ y cÅ©ng khiáº¿n mÃ´
hÃ¬nh bá»‹ vanishing gradient.
=> Cáº§n má»™t bÆ°á»›c normalize dá»¯ liá»‡u trÆ°á»›c khi Ä‘i qua hÃ m activation.
HÃ¬nh 12.12: Äá»“ thá»‹ cÃ¡c hÃ m activation
Batch Normalization ra Ä‘á»i 6.2
Batch normalization thá»±c hiá»‡n viá»‡c chuáº©n hÃ³a (normalizing) vÃ  zero centering (mean
substracting) dá»¯ liá»‡u trÆ°á»›c khi Ä‘Æ°a qua hÃ m activation (giÃ¡ trá»‹ trung bÃ¬nh (mean) sáº½ Ä‘Æ°á»£c Ä‘Æ°a
vá» 0 vÃ  phÆ°Æ¡ng sai (variance) sáº½ Ä‘Æ°á»£c Ä‘Æ°a vá» 1). Äá»ƒ thá»±c hiá»‡n 2 cÃ´ng viá»‡c trÃªn, batch
normalization tÃ­nh toÃ¡n phÆ°Æ¡ng sai vÃ  Ä‘á»™ lá»‡ch chuáº©n cá»§a dá»¯ liá»‡u dá»±a trÃªn cÃ¡c batchs, rá»“i sá»­
dá»¥ng 2 tham sá»‘ Î³ vÃ  Î² tinh chá»‰nh Ä‘áº§u ra.
Batch normalization:
ÂµB x(i)
xË†
z(i) =Î³xË†+Î²
Trong Ä‘Ã³: ÂµB lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a batch B
Ïƒ2B lÃ  phÆ°Æ¡ng sai cá»§a batch B
xË†(i) lÃ  giÃ¡ trá»‹ cá»§a máº«u thá»© i trong batch B sau khi Ä‘Æ°á»£c normalize vÃ  zero centering
z(i) lÃ  Ä‘áº§u ra cá»§a máº«u thá»© i trong batch B
Î³ lÃ  scaling parameter cá»§a lá»›p
Î² lÃ  shifting parameter cá»§a lá»›p
Î¾ lÃ  smoothing parameter, trÃ¡nh xáº£y ra viá»‡c chia cho 0, giÃ¡ trá»‹ ráº¥t nhá»
ChÃº Ã½: Î³ vÃ  Î² lÃ  2 tham sá»‘ Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh training.
Hiá»‡u quáº£ cá»§a batch normalization 6.3
Batch normalization Ä‘Æ°a dá»¯ liá»‡u vá» zero mean vÃ  chuáº©n hÃ³a variance trÆ°á»›c khi Ä‘Æ°a â€¢
qua activation function nhá» Ä‘Ã³ giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» vanishing gradient hay exploding
gradient.
Batch normalization cho phÃ©p learning rate lá»›n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. â€¢
Batch-Norm giáº£m thiá»ƒu sá»± áº£nh hÆ°á»Ÿng cá»§a quÃ¡ trÃ¬nh khá»Ÿi táº¡o trá»ng sá»‘ ban Ä‘áº§u. â€¢
Batch-Norm chuáº©n hÃ³a dá»¯ liá»‡u Ä‘áº§u ra cá»§a cÃ¡c layer giÃºp model trong quÃ¡ trÃ¬nh huáº¥n â€¢
luyá»‡n khÃ´ng bá»‹ phá»¥ thuá»™c vÃ o má»™t thÃ nh pháº§n trá»ng sá»‘ nháº¥t Ä‘á»‹nh. Do Ä‘Ã³, Batch-norm cÃ²n
Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t regularizer giÃºp giáº£m overfitting